{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brown.categories() brown语料库文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brown.sents()句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents()) # 句子总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()#包含的单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())#单词总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本处理流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档导入-->文本预处理-->分词-->特征提取-->ML-->Label/Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"hello, world\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词的两种方式: 启发式Heuristic 和机器学习/统计方法:HMM,CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启发式: 如 今天天气不错--> 今天/天天/天气/不错\n",
    "统计方法: 是依据统计方法统计词出现频率来分词."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中英文分词差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文分词的三种模式: 全模式,精确模式,搜索模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/\".join(seg_list)) #全模式\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"False Mode:\", \"/\".join(seg_list)) #精确模式\n",
    "seg_list = jieba.cut_for_search(\"我来到北京清华大学\")\n",
    "print(\"Search Mode:\", \"/\".join(seg_list)) #全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Full Mode: 我/来到/北京/清华/清华大学/华大/大学\n",
    "#False Mode: 我/来到/北京/清华大学\n",
    "#Search Mode: 我/来到/北京/清华/华大/大学/清华大学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "社交网络语言的tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则表达式\n",
    "对照表http://www.regexlab.com/zh/regref.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "#print(word_tokenize(tweet))\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "            [:=;] # 眼睛\n",
    "            [oO\\-]? # ⿐鼻⼦子\n",
    "            [D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @某⼈人\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',# URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词\n",
    "    r'(?:[\\w_]+)', # 其他\n",
    "    r'(?:\\S)' # 其他\n",
    "]\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 纷繁复杂的词形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflection变化: walk => walking => walked\n",
    "不影响词性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivation 引申: nation (noun) => national (adjective) => nationalize (verb)\n",
    "影响词性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词形归⼀化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stem词干提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming 词⼲提取：⼀般来说，就是把不影响词性的inflection的⼩尾巴砍掉\n",
    "walking 砍ing = walk\n",
    "walked 砍ed = walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "maxim\n",
      "maximum\n",
      "presum\n",
      "presum\n",
      "presum\n",
      "multipli\n",
      "multiply\n",
      "multipli\n",
      "provis\n",
      "provid\n",
      "provis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "p = PorterStemmer()\n",
    "l = LancasterStemmer()\n",
    "s = SnowballStemmer(language=\"english\") # 这个必须指定语言language\n",
    "words = ['maximum','presumably','multiply','provision']\n",
    "for word in words:\n",
    "    print(p.stem(word))\n",
    "    print(l.stem(word))\n",
    "    print(s.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatization词形归一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization 词形归⼀：把各种类型的词的变形，都归为⼀个形式\n",
    "went 归⼀ = go\n",
    "are 归⼀ = be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "church\n",
      "aardwolf\n",
      "abacus\n",
      "hardrock\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "words = ['dogs','churches','aardwolves','abaci','hardrock']\n",
    "for word in words:\n",
    "    print(wordnet_lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma的小问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Went 有可能是go的过去式;也有可能是温特的名字\n",
    "因此,更好地实现Lemma词形归一需要运用到时态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 没有POS Tag,默认是NN名词\n",
    "wordnet_lemmatizer.lemmatize(\"are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize(\"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加上POS Tag\n",
    "wordnet_lemmatizer.lemmatize(\"are\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize(\"is\", pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK标注POS Tag词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'does', 'the', 'fox', 'say']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize('what does the fox say')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WDT'),\n",
       " ('does', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('fox', 'NNS'),\n",
       " ('say', 'VBP')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text) #有错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopWord停用词\n",
    "全体stopwords列表 http://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⾸先记得在console⾥⾯下载⼀下词库\n",
    "或者 nltk.download(‘stopwords’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Oh, baby with your pretty face　　\n",
    "Drop a tear in my wineglass　　\n",
    "Look at those big eyes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh',\n",
       " ',',\n",
       " 'baby',\n",
       " 'pretty',\n",
       " 'face',\n",
       " 'Drop',\n",
       " 'tear',\n",
       " 'wineglass',\n",
       " 'Look',\n",
       " 'big',\n",
       " 'eyes']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [word for word in word_list if word not in stopwords.words(\"english\")]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⼀条typical的⽂本预处理流⽔线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Raw_Text\n",
    "#   |\n",
    "# Tokenize  --->POS Tag\n",
    "#   |       /\n",
    "# Lemma/Stemming\n",
    "#   |\n",
    "# stopwords\n",
    "#   |\n",
    "# Word_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK在NLP上的经典应⽤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析,⽂本相似度,⽂本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 情感分析\n",
    "最简单的 sentiment dictionary\n",
    "如:\n",
    "like 1 \n",
    "good 2 \n",
    "bad -2 \n",
    "terrible -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似于关键词打分机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⽐如：AFINN-111\n",
    "http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK完成简单的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dictionary = {}\n",
    "for line in open(r'C:\\study\\datasets\\NLP\\imm6010\\AFINN\\AFINN-111.txt'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)\n",
    "# 把这个打分表记录在⼀一个Dict上以后\n",
    "# 跑⼀一遍整个句句⼦子，把对应的值相加\n",
    "#print(sentiment_dictionary)\n",
    "words = \"\"\"Oh, baby with your pretty face　　\n",
    "Drop a tear in my wineglass　　\n",
    "Look at those big eyes\n",
    "\"\"\"\n",
    "words = nltk.word_tokenize(words)\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in words)\n",
    "# 有值就是Dict中的值，没有就是0\n",
    "# 于是你就得到了了⼀一个 sentiment score\n",
    "total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 配上ML的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "# 随⼿手造点训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "\n",
    "def preprocess(s):\n",
    "    # Func: 句句⼦子处理理\n",
    "    # 这⾥里里简单的⽤用了了split(), 把句句⼦子中每个单词分开\n",
    "    # 显然 还有更更多的processing method可以⽤用\n",
    "    return {word: True for word in s.lower().split()}\n",
    "    # return⻓长这样:\n",
    "    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}\n",
    "    # 其中, 前⼀一个叫fname, 对应每个出现的⽂文本单词;\n",
    "    # 后⼀一个叫fval, 指的是每个⽂文本单词对应的值。\n",
    "    # 这⾥里里我们⽤用最简单的True,来表示,这个词『出现在当前的句句⼦子中』的意义。\n",
    "    # 当然啦, 我们以后可以升级这个⽅方程, 让它带有更更加⽜牛逼的fval, ⽐比如 word2vec\n",
    "    # 把训练集给做成标准形式\n",
    "training_data = [[preprocess(s1), 'pos'],\n",
    "[preprocess(s2), 'pos'],\n",
    "[preprocess(s3), 'neg'],\n",
    "[preprocess(s4), 'neg']]\n",
    "# 喂给model吃\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "# 打出结果\n",
    "print(model.classify(preprocess('this is a good book')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⽤元素频率表⽰⽂本特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 余弦定理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosβ=A*B/(|A| |B|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency频率统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'my', 'sentencethis', 'is', 'my', 'lifethis', 'is', 'the', 'day']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# 做个词库先\n",
    "corpus = 'this is my sentence' \\\n",
    "'this is my life' \\\n",
    "'this is the day'\n",
    "\n",
    "# 随便分词tokenize一下\n",
    "# 显然,正如上文提到,这里可以根据需要做任何的preprocessing: stopwords,lemma, stemming, etc.\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'day': 1,\n",
       "          'is': 3,\n",
       "          'lifethis': 1,\n",
       "          'my': 2,\n",
       "          'sentencethis': 1,\n",
       "          'the': 1,\n",
       "          'this': 1})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 借用NLTK的FreqDist统计一下文字出现的频率,并没有排序\n",
    "fdist = FreqDist(tokens)\n",
    "fdist\n",
    "# 它就类似于一个Dist\n",
    "# 带上某个单词,可以看到它在整个文章中出现的次数\n",
    "#print(fdist['is'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把最常用的n个单词拿出来FreqDist.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 3), ('my', 2), ('this', 1), ('sentencethis', 1), ('lifethis', 1)]\n"
     ]
    }
   ],
   "source": [
    "standard_freq_vector = fdist.most_common(5) # 排序好的\n",
    "size = len(standard_freq_vector)\n",
    "print(standard_freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'my': 1, 'this': 2, 'sentencethis': 3, 'lifethis': 4}\n"
     ]
    }
   ],
   "source": [
    "# Func: 按照出现频率大小,记录下每一个单词的位置\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter += 1\n",
    "    return res\n",
    "\n",
    "# 把标准的单词位置记录下来\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "print(standard_position_dict) #{'is': 0, 'lifethis': 4, 'my': 1, 'sentencethis': 3, 'this': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这时, 如果我们有一个新句子:\n",
    "sentence = 'this is cool'\n",
    "# 先新建一个跟我们的标准vector同样大小的向量\n",
    "freq_vector = [0] * size\n",
    "# 简单的Preprocessing\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "# 对于这个新句子里的每一个单词\n",
    "for word in tokens:\n",
    "    try:\n",
    "        # 如果在我们的词库里出现过\n",
    "        # 那么久在标准位置上+1\n",
    "        print(standard_position_dict[word])\n",
    "        freq_vector[standard_position_dict[word]] += 1\n",
    "    except KeyError:\n",
    "        # 如果是个新词\n",
    "        # 就pass掉\n",
    "        continue\n",
    "print(freq_vector)#[1, 0, 1, 0, 0]\n",
    "# 第一个位置代表is 出现了一次\n",
    "# 第三个位置代表this,出现了一次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本分类\n",
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TF: TremFrequency, 衡量一个term在文档中出现的多频繁.\n",
    "TF(t) = (t出现在文档中的次数)/(文档中的term总数)\n",
    "IDF: Inverse Document Frequency, 衡量一个term有多重要.有些词出现的很多,但是明显不是很有卵用,比如is,the之类.\n",
    "为了平衡,我们把罕见的词的重要性(权重weight)提高,把常见词的重要性降低.\n",
    "IDF(t) = log_e(文档总数/含有t的文档总数)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF = TF* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 举个栗子\n",
    "# TF: 一个文档中有100个单词,其中单词baby出现了3次, 那么TF(baby)=3/100 = 0.03\n",
    "# IDF: 我们现在有10 million个文档,baby出现在其中的1000个文档中.\n",
    "# 那么IDF(baby) = log(10,000,000/1,000)=4\n",
    "# 所以TF-IDF(baby) = TF(baby)*IDF(baby) = 0.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK 实现TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextCollection 断句,统计,计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "# 首先, 把所有的文档放到TextCollection类中.\n",
    "# 这个类会自动帮你断句,做统计, 做计算\n",
    "corpus = TextCollection(['this is'])\n",
    "print(len(corpus))\n",
    "# 直接就能算出tfidf\n",
    "# (term: 一句话中某个term, text:这句话)\n",
    "print(corpus.tf_idf('is','this is sentence four'))\n",
    "corpus.idf('is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意TextCollection源码中tfidf的计算公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf(self, term, text):\n",
    "#         \" The frequency of the term in text. \"\n",
    "#         return text.count(term) / len(text)\n",
    "\n",
    "\n",
    "# def idf(self, term):\n",
    "#         \"\"\" The number of texts in the corpus divided by the\n",
    "#         number of texts that the term appears in.\n",
    "#         If a term does not appear in the corpus, 0.0 is returned. \"\"\"\n",
    "#         # idf values are cached for performance.\n",
    "#         idf = self._idf_cache.get(term)\n",
    "#         if idf is None:\n",
    "#             matches = len([True for text in self._texts if term in text])\n",
    "#             if len(self._texts) == 0:\n",
    "#                 raise ValueError('IDF undefined for empty document collection')\n",
    "#             idf = log(len(self._texts) / matches) if matches else 0.0\n",
    "#             self._idf_cache[term] = idf\n",
    "#         return idf\n",
    "\n",
    "\n",
    "# def tf_idf(self, term, text):\n",
    "#         return self.tf(term, text) * self.idf(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextCollection可以直接计算tf,idf,tf_idf值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import TextCollection\n",
    "corpus = TextCollection(['this is sentence one','this is sentence two','this is sentence three'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047619047619047616"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.tf(\"this is\",\"this is sentence four\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.idf(\"this is\") # 结果是0, log(3/3)=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.tf_idf(\"this is\",\"this is sentence four\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML可能的ML模型: SVM LR RF MLP LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
